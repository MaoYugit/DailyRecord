### **第二课：核心运算 (上) —— 让数据动起来！**

**目标:** 掌握操纵数据的基本“动作”：加法、数乘和至关重要的**点积**。我们将看到点积如何成为衡量“相似度”的强大工具。

现在我们有了数据（向量和矩阵），就像有了棋盘上的棋子。接下来，我们要学习移动这些棋子的规则。

#### **1. 最简单的移动：向量/矩阵加法与标量乘法**

这两种运算非常直观，它们都遵循一个简单的原则：**按元素 (element-wise)** 进行。

**A. 向量/矩阵加法 (Vector/Matrix Addition)**

*   **是什么：** 将两个形状完全相同的向量或矩阵，对应位置的元素相加。
*   **规则：** **形状必须完全匹配！** 一个 2x3 的矩阵只能和另一个 2x3 的矩阵相加。你不能把一个用户向量（含3个特征）和一个商品向量（含5个特征）直接相加，因为它们的维度不匹配，在数学和现实意义上都没有道理。
*   **直观理解（几何意义）：** 想象两个二维向量 `a = [1, 3]` 和 `b = [2, 1]`。
    *   `a` 是从原点指向 (1, 3) 的箭头。
    *   `b` 是从原点指向 (2, 1) 的箭头。
    *   `a + b` 的结果是 `[1+2, 3+1] = [3, 4]`。在几何上，这相当于将向量 `b` 的尾部平移到向量 `a` 的头部，然后画一个从原点到 `b` 新头部的箭头。这就是物理学中力的合成（平行四边形法则）。
*   **在AI中：**
    *   **神经网络中的偏置项：** 在神经网络的一层中，输入数据经过权重矩阵变换后，通常会加上一个**偏置向量 (bias vector)**，这个过程就是向量加法。它的作用是给变换后的数据一个整体的平移，增加模型的灵活性。
    *   **图像处理：** 两张同样大小的图片（矩阵）可以相加，实现图像融合的效果。

**B. 标量乘法 (Scalar Multiplication)**

*   **是什么：** 将一个标量（单个数字）与一个向量或矩阵中的**每一个元素**相乘。
*   **直观理解（几何意义）：** 这相当于对一个向量进行**缩放 (scaling)**。
    *   如果我们有向量 `v = [2, 1]`，那么 `3 * v` 的结果是 `[6, 3]`。
    *   在几何上，这还是一个指向**相同方向**的箭头，但它的**长度变成了原来的3倍**。
    *   如果乘以一个负数（如 -1），则会使箭头的方向完全反转。
*   **在AI中：**
    *   **调整权重：** 在模型训练中，算法会根据误差来微调网络中的权重，这个微调过程通常就是用一个很小的标量（学习率）乘以一个梯度向量/矩阵。
    *   **数据归一化：** 在预处理数据时，我们可能会将某个特征的所有值都乘以一个系数，以将其缩放到特定范围内。

#### **2. 核心中的核心：点积 (Dot Product)**

现在，我们来看一个看似简单却极其深刻的运算。点积是向量与向量之间的一种“乘法”，它是理解几乎所有高级AI概念（包括神经网络和相似度计算）的基石。

*   **是什么：** 两个**维度相同**的向量的点积，是它们对应元素相乘后再求和的结果。**结果是一个标量（单个数字）**。
    *   给定 `a = [a₁, a₂, ..., aₙ]` 和 `b = [b₁, b₂, ..., bₙ]`
    *   `a · b = a₁b₁ + a₂b₂ + ... + aₙbₙ`
*   **计算示例：**
    *   `a = [1, 2, 3]`
    *   `b = [4, 5, 6]`
    *   `a · b = (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32`
*   **直观理解与几何意义 (至关重要！)：**
    点积的数值大小，告诉了我们这两个向量在方向上的“一致程度”。它的几何定义是：`a · b = |a| * |b| * cos(θ)`
    *   `|a|` 和 `|b|` 是两个向量的长度（模）。
    *   `θ` 是两个向量之间的夹角。
    *   `cos(θ)` 是关键：
        *   当两个向量方向**完全相同** (θ=0°)，`cos(θ)=1`，点积达到最大正值。它们“高度相关”。
        *   当两个向量**相互垂直** (θ=90°)，`cos(θ)=0`，点积为 **0**。这在数学上称为**正交 (orthogonal)**。它们“线性无关”。
        *   当两个向量方向**完全相反** (θ=180°)，`cos(θ)=-1`，点积达到最大负值。它们“高度负相关”。
*   **在AI中 (应用无处不在！)：**
    *   **衡量相似度：** 这是点积最直接、最广泛的应用！如果我们将用户、文档、商品都表示为向量，那么计算它们之间的点积（或其变种**余弦相似度** `cos(θ) = (a·b) / (|a||b|)`）就可以知道它们的相似程度。
        *   **推荐系统：** 用户A的喜好向量 `[5, 4, 0, 1]` (5分科幻, 4分动作, 0分爱情, 1分喜剧) 和用户B的喜好向量 `[4, 5, 1, 0]` 点积会很大，说明他们品味相似，可以互相推荐电影。
        *   **搜索引擎：** 你的搜索查询会被转换成一个向量，搜索引擎会计算它与海量文档向量的点积，返回点积最大的那些文档。
    *   **神经网络的计算基础：** 神经网络中一个神经元的计算过程，本质上就是将其输入向量与它的权重向量进行**点积**运算，再加上一个偏置，最后通过激活函数。点积在这里的作用是：衡量输入特征与神经元“关注”的特征模式有多匹配。如果匹配度高，点积就大，神经元就会被“激活”。

---

### **动手实践：NumPy 中的运算**

```python
import numpy as np

# -- 加法与数乘 --
# 假设我们有两个用户的特征向量：[网页浏览时长(分钟)，购买次数]
user_stats_1 = np.array([50, 3])
user_stats_2 = np.array([20, 8])

# 1. 向量加法: 得到总计的统计数据
total_stats = user_stats_1 + user_stats_2
print(f"向量相加: {user_stats_1} + {user_stats_2} = {total_stats}")

# 2. 标量乘法: 假设我们要预测一周的浏览时长
weekly_browsing_1 = user_stats_1[0] * 7
print(f"用户1的周浏览时长预测: {user_stats_1[0]} * 7 = {weekly_browsing_1}")
print("-" * 30)


# -- 点积 --
# 假设我们有三个用户的电影评分向量：[科幻, 动作, 爱情]
# 评分范围1-5
user_A = np.array([5, 4, 1]) # 喜欢科幻、动作
user_B = np.array([4, 5, 0]) # 和A类似
user_C = np.array([1, 0, 5]) # 喜欢爱情片

# 计算A和B的点积
dot_AB = np.dot(user_A, user_B)
print(f"用户A和B的点积: {dot_AB}") # 期待一个比较大的正数

# 计算A和C的点积
dot_AC = np.dot(user_A, user_C)
print(f"用户A和C的点积: {dot_AC}") # 期待一个比较小的数

# 我们可以用 @ 符号进行更简洁的点积/矩阵乘法运算
dot_BC = user_B @ user_C
print(f"用户B和C的点积 (用@): {dot_BC}")

print("\n结论：点积越大，代表用户品味越相似。推荐系统可以据此为A推荐B喜欢的电影。")
```

---

### **第一份作业**

为了巩固今天的知识，请尝试完成以下任务：

**1. 概念思考题：**
    a. 为什么在进行向量/矩阵加法时，它们的形状必须完全相同？请从“特征”的角度举一个现实生活中的例子来解释。

>从更数学化的角度来说，加法是定义在**相同维度空间**里的运算。向量 [身高, 体重] 是一个二维空间中的点，而向量 [年龄, 收入, 信用分] 是三维空间中的一个点。试图将这两个点相加，在几何上和逻辑上都是没有意义的。每个位置（索引）都必须对齐，因为它代表着同一个“坐标轴”或“特征维度”。

​    b. 两个非零的商品特征向量，它们的点积为0，这在“推荐系统”的场景下可能意味着什么？

> 两个非两个非零的商品特征向量，它们的点积为0，这在“推荐系统”的场景下表示他们之间完全不相关。“不相关”在几何上的体现就是**正交 (Orthogonal)**，也就是我们常说的“相互垂直”。在推荐系统的场景下，这意味着一个商品的核心特征（比如，商品A是“硬核科幻小说”）与另一个商品的核心特征（比如，商品B是“幼儿启蒙绘本”）毫不重叠。这两个商品的特征向量在特征空间中指向了完全不同的方向。

**2.  NumPy 编程题：**
    假设你正在为一个在线课程平台设计一个简单的课程推荐功能。我们有两位学生（A和B）和三个课程维度：`[数学, 编程, 艺术]`。他们的兴趣程度用一个1-10的向量表示。

    *   学生A的兴趣向量: `student_A = [8, 9, 2]`
    *   学生B的兴趣向量: `student_B = [3, 4, 9]`
    
    a. 有一个新的“数据科学”课程包，它在三个维度上的增益是 `gain = [2, 3, 1]`。请用向量加法计算出两位学生学习该课程包后，新的兴趣向量是什么。
    b. 平台决定将所有学生的“编程”兴趣分值加倍以鼓励他们学习编程。请用标量乘法和向量加法（或者只用标量乘法和逐元素操作）来实现这个更新，并输出两位学生更新后的兴趣向量。（提示：你可能需要创建一个只在特定位置有值的向量）。
    c. 计算学生A和学生B原始兴趣向量的点积。这个结果是高还是低？这说明了什么？

```python
student_A = np.array([8, 9, 2])
student_B = np.array([3, 4, 9])

gain = np.array([2, 3, 1])

student_A_new = student_A + gain
student_B_new = student_B + gain

print(f"student_A学习数据科学课程包后，新的兴趣向量是：{student_A_new}")
print(f"student_B学习数据科学课程包后，新的兴趣向量是：{student_B_new}")

# 创建副本以避免修改原始向量
student_A_updated_v1 = student_A.copy()
student_B_updated_v1 = student_B.copy()

# “编程”在索引1的位置
student_A_updated_v1[1] = student_A_updated_v1[1] * 2
student_B_updated_v1[1] = student_B_updated_v1[1] * 2

print(f"方法一：学生A更新后的向量: {student_A_updated_v1}") # 结果应为 [8, 18, 2]
print(f"方法一：学生B更新后的向量: {student_B_updated_v1}") # 结果应为 [3, 8, 9]

dot_AB = student_A @ student_B
print(f"学生A和学生B原始兴趣向量的点积:{dot_AB}")
```

接下来，我们将进入本模块最核心的部分：**矩阵乘法**。它建立在点积之上，是理解神经网络数据流动的关键一步。你准备好了吗？